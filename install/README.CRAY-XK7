Info by Filippo Spiga  -- Sept 18, 2013

Machine name    : TITAN at Oak Ridge National laboratory (USA)
Machine spec    : https://www.olcf.ornl.gov/computing-resources/titan-cray-xk7/

# IMPORTANT NOTE: 
Other CRAY XK7 systems might have different modules, please check for 
equivalent if the ones mentioned are missing



0. Architecture peculiarities

CRAY XK7 systems currently in operation are equipped with one AMD 16-core
Interlagos and one NVIDIA K20x. The XK7 blade layout is then similar to the
CRAY XE6 one.

As remind, Interlagos is composed of a number of "Bulldozer modules" or 
"Compute Unit". A compute unit has shared and dedicated components:
- there are two independent integer units
- a SHARED 256-bit Floating Point pipeline supporting SSEx and AVX extension

For this reason it is better to use the CPU in SINGLE STREAM MODE (aprun -j 1)
reducing the maximum number of OpenMP thread per node from 16 to 8 being able
to exploit at maximum the floating-point pipeline. In this way the L2 cache 
is effectively twice as large and the peak performance (in double-precision)
should not be affected.

The interconnection topology might have "holes" due to service nodes and I/O 
nodes. Cray's Application Level Placement Scheduler (ALPS) should be able to
support a resource manager to identify a subset of free nodes in the cluster 
to minimize hops. Please refer to specific user-guide provided by your HPC
centre.



1. Compile GPU-accelerated PWscf

Up to now, extensive tests proven that Intel compiler is the best choice now 
to exploit GPU capabilities of QE-GPU on CRAY XK7 nodes. 

# NOTE: 
Despite the selected compiler is Intel, xt-libsci (usually 12.0.02 or 12.0.03)
is used.

Intel:
- compile: ok
- CPU execution: ok
- GPU execution: ok

PGI:
- compile: ok
- CPU execution: ok
- GPU execution: ok

GNU: not tested yet
        
_ After login to a system....

module load cudatoolkit
module unload atp totalview-support xt-totalview hss-llm

export FINALDIR=$HOME/whatever
mkdir -p $FINALDIR

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --without-magma --with-phigemm  --with-scalapack ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-scalapack.x

_or_ 

make -f Makefile.gpu distclean
cd GPU/
./configure --enable-parallel --enable-openmp --enable-cuda --with-gpu-arch=35 --with-cuda-dir=${CRAY_CUDATOOLKIT_DIR} --without-magma --with-phigemm  --with-scalapack --with-elpa ARCH=crayxt
cd ../
make -f Makefile.gpu pw-gpu
cp GPU/PW/pw-gpu.x ${FINALDIR}/pw-mpi-omp-gpu-elpa.x


IMPORTANT NOTE (1): in case of PGI compiler, it is possible to compile a light version 
                    of the executable by avoiding CRAY wrappers. Look at the file 
                    "" as example.

IMPORTANT NOTE (2): On TITAN the PGI compiler is the default right after login. If
                    Intel is the preferred choice, do
                    $ module switch PrgEnv-pgi PrgEnv-intel
                    immediately after the login.



3. Running (using TORQUE/PBS Pro)

Refer to this link 
   https://www.olcf.ornl.gov/kb_articles/spider-the-center-wide-lustre-file-system/
about the filesystem management and availability

IMPORTANT NOTE (1): 
The per node charging factor changed from 16 to 30 to reflect the availability
of GPU/Accelerators. Job utilization is now calculated via the formula: 
      30 * wall-time * number of requested nodes 

IMPORTANT NOTE (2): 
Project granted at ORNL usually have 3 letters (XXX) and three digits (YYY)


#!/bin/csh
#
# Example requesting 8 nodes (64 cores in total in SINGLE STREAM MODE 
# using 8 OpenMP thread per MPI), 1 MPI process per node (8 in total)
# dedicating the full NVIDIA K20x resource to a single MPI process.
#
#PBS -A <XXXYYY>
#PBS -N QE-BENCH
#PBS -j oe
#PBS -l walltime=1:00:00,nodes=8

cd $PBS_O_WORKDIR

# Enable _only_ if '-N' > 1 (see below)
#setenv CRAY_CUDA_PROXY 1

# DEBUG
#setenv MPICH_ENV_DISPLAY 1

# _REMEMBER_
# '-n' : number of PEs or total MPI processes
# '-d' : number OpenMP thread per node
# '-N' : number of MPI processes per node
# '-j' : 
 
setenv OMP_NUM_THREADS 8
aprun -N 1 -n 8 -j 1 -d 8 -cc numa_node ./pw-mpi-omp-gpu.x -input ausurf_gamma.in | tee out.GPU.1-PER-NODE.$PBS_JOBID.v1



2. Good practices

- Each NVIDIA Tesla K20 GPU has 6 GB of memory on the card. Better to limit 
  the number of MPI per node (so the number of MPI sharing the same GPU) 
  to 2.

- If the calculation is not too memory demanding, it is possible to increase 
  the ratio MPI:GPU up to 4.
  
- In order to share the GPU between multiple MPI processes within the node is 
  mandatory to export the variable CRAY_CUDA_PROXY ("export CRAY_CUDA_PROXY=1")

- compiling with huge-pages support does not produce big benefits, need 
  more testing...



5. Benchmarks 

[TO BE ADDED]